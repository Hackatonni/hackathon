[
    {
        "id": "705.3597",
        "title": "DNA Hash Pooling and its Applications",
        "abstract": "  In this paper we describe a new technique for the comparison of populations\nof DNA strands. Comparison is vital to the study of ecological systems, at both\nthe micro and macro scales. Existing methods make use of DNA sequencing and\ncloning, which can prove costly and time consuming, even with current\nsequencing techniques. Our overall objective is to address questions such as:\n(i) (Genome detection) Is a known genome sequence present, at least in part, in\nan environmental sample? (ii) (Sequence query) Is a specific fragment sequence\npresent in a sample? (iii) (Similarity discovery) How similar in terms of\nsequence content are two unsequenced samples? We propose a method involving\nmultiple filtering criteria that result in \"pools\" of DNA of high or very high\npurity. Because our method is similar in spirit to hashing in computer science,\nwe call it DNA hash pooling. To illustrate this method, we describe protocols\nusing pairs of restriction enzymes. The in silico empirical results we present\nreflect a sensitivity to experimental error. Our method will normally be\nperformed as a filtering step prior to sequencing in order to reduce the amount\nof sequencing required (generally by a factor of 10 or more). Even as\nsequencing becomes cheaper, an order of magnitude remains important.\n",
        "submitter": "Dennis Shasha",
        "authors": "Dennis Shasha (Courant Institute, New York University) and Martyn Amos\n  (Computing and Mathematics, Manchester Metropolitan University)",
        "comments": "14 pages, 3 figures. To appear in the International Journal of\n  Nanotechnology and Molecular Computation. Improved background, analysis and\n  references",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "q-bio.BM q-bio.PE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2008-07-02",
        "authors_parsed": "Shasha Dennis Courant Institute, New York University; Amos Martyn Computing and Mathematics, Manchester Metropolitan University"
    },
    {
        "id": "710.3901",
        "title": "A recursive linear time modular decomposition algorithm via LexBFS",
        "abstract": "  A module of a graph G is a set of vertices that have the same set of\nneighbours outside. Modules of a graphs form a so-called partitive family and\nthereby can be represented by a unique tree MD(G), called the modular\ndecomposition tree. Motivated by the central role of modules in numerous\nalgorithmic graph theory questions, the problem of efficiently computing MD(G)\nhas been investigated since the early 70's. To date the best algorithms run in\nlinear time but are all rather complicated. By combining previous algorithmic\nparadigms developed for the problem, we are able to present a simpler\nlinear-time that relies on very simple data-structures, namely slice\ndecomposition and sequences of rooted ordered trees.\n",
        "submitter": "Christophe Paul",
        "authors": "Derek Corneil and Michel Habib and Christophe Paul and Marc Tedder",
        "comments": "An EA of this work appeared in ICALP'08. The arXiv v2 contains an\n  appendix with some sketches of proofs. To date, complete proofs can only be\n  found in the PhD of M. Tedder and spread over several chapters. This is a\n  self-contained version. To ease the understanding, the noveI presentation\n  enlights the combinatorial objects involved in the algorithm, which still\n  relies on the same ideas",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.DM",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "update_date": "2024-07-15",
        "authors_parsed": "Corneil Derek; Habib Michel; Paul Christophe; Tedder Marc"
    },
    {
        "id": "801.0848",
        "title": "Batch kernel SOM and related Laplacian methods for social network\n  analysis",
        "abstract": "  Large graphs are natural mathematical models for describing the structure of\nthe data in a wide variety of fields, such as web mining, social networks,\ninformation retrieval, biological networks, etc. For all these applications,\nautomatic tools are required to get a synthetic view of the graph and to reach\na good understanding of the underlying problem. In particular, discovering\ngroups of tightly connected vertices and understanding the relations between\nthose groups is very important in practice. This paper shows how a kernel\nversion of the batch Self Organizing Map can be used to achieve these goals via\nkernels derived from the Laplacian matrix of the graph, especially when it is\nused in conjunction with more classical methods based on the spectral analysis\nof the graph. The proposed method is used to explore the structure of a\nmedieval social network modeled through a weighted graph that has been directly\nbuilt from a large corpus of agrarian contracts.\n",
        "submitter": "Nathalie Villa",
        "authors": "Romain Boulet (IMT), Bertrand Jouve (IMT), Fabrice Rossi (INRIA\n  Rocquencourt / INRIA Sophia Antipolis), Nathalie Villa (IMT)",
        "comments": NaN,
        "journal-ref": "Neurocomputing / EEG Neurocomputing (2008) A para\\^itre",
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.AP math.ST stat.ME stat.ML stat.TH",
        "license": NaN,
        "update_date": "2008-01-08",
        "authors_parsed": "Boulet Romain IMT; Jouve Bertrand IMT; Rossi Fabrice INRIA\n  Rocquencourt / INRIA Sophia Antipolis; Villa Nathalie IMT"
    },
    {
        "id": "801.1647",
        "title": "Extending the definition of modularity to directed graphs with\n  overlapping communities",
        "abstract": "  Complex networks topologies present interesting and surprising properties,\nsuch as community structures, which can be exploited to optimize communication,\nto find new efficient and context-aware routing algorithms or simply to\nunderstand the dynamics and meaning of relationships among nodes. Complex\nnetworks are gaining more and more importance as a reference model and are a\npowerful interpretation tool for many different kinds of natural, biological\nand social networks, where directed relationships and contextual belonging of\nnodes to many different communities is a matter of fact. This paper starts from\nthe definition of modularity function, given by M. Newman to evaluate the\ngoodness of network community decompositions, and extends it to the more\ngeneral case of directed graphs with overlapping community structures.\nInteresting properties of the proposed extension are discussed, a method for\nfinding overlapping communities is proposed and results of its application to\nbenchmark case-studies are reported. We also propose a new dataset which could\nbe used as a reference benchmark for overlapping community structures\nidentification.\n",
        "submitter": "Vincenzo Nicosia",
        "authors": "V. Nicosia, G. Mangioni, V. Carchiolo and M. Malgeri",
        "comments": "22 pages, 11 figures",
        "journal-ref": "J. Stat. Mech. (2009) P03024",
        "doi": "10.1088/1742-5468/2009/03/P03024",
        "report-no": NaN,
        "categories": "physics.data-an physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-03-24",
        "authors_parsed": "Nicosia V.; Mangioni G.; Carchiolo V.; Malgeri M."
    },
    {
        "id": "801.28",
        "title": "A preferential attachment model with Poisson growth for scale-free\n  networks",
        "abstract": "  We propose a scale-free network model with a tunable power-law exponent. The\nPoisson growth model, as we call it, is an offshoot of the celebrated model of\nBarab\\'{a}si and Albert where a network is generated iteratively from a small\nseed network; at each step a node is added together with a number of incident\nedges preferentially attached to nodes already in the network. A key feature of\nour model is that the number of edges added at each step is a random variable\nwith Poisson distribution, and, unlike the Barab\\'{a}si-Albert model where this\nquantity is fixed, it can generate any network. Our model is motivated by an\napplication in Bayesian inference implemented as Markov chain Monte Carlo to\nestimate a network; for this purpose, we also give a formula for the\nprobability of a network under our model.\n",
        "submitter": "Paul Sheridan",
        "authors": "Paul Sheridan, Yuichi Yagahara and Hidetoshi Shimodaira",
        "comments": "18 pages with 2 figures; correction to a proof in the appendix",
        "journal-ref": "Annals of the Institute of Statistical Mathematics 2008, Vol. 60,\n  pp. 747-761",
        "doi": "10.1007/s10463-008-0181-5",
        "report-no": NaN,
        "categories": "stat.AP",
        "license": NaN,
        "update_date": "2013-12-24",
        "authors_parsed": "Sheridan Paul; Yagahara Yuichi; Shimodaira Hidetoshi"
    },
    {
        "id": "802.0543",
        "title": "Improving Performance of Cluster Based Routing Protocol using\n  Cross-Layer Design",
        "abstract": "  The main goal of routing protocol is to efficiency delivers data from source\nto destination. All routing protocols are the same in this goal, but the way\nthey adopt to achieve it is different, so routing strategy has an egregious\nrole on the performance of an ad hoc network. Most of routing protocols\nproposed for ad hoc networks have a flat structure. These protocols expand the\ncontrol overhead packets to discover or maintain a route. On the other hand a\nnumber of hierarchical-based routing protocols have been developed, mostly are\nbased on layered design. These protocols improve network performances\nespecially when the network size grows up since details about remote portion of\nnetwork can be handled in an aggregate manner. Although, there is another\napproach to design a protocol called cross-layer design. Using this approach\ninformation can exchange between different layer of protocol stack, result in\noptimizing network performances.\n  In this paper, we intend to exert cross-layer design to optimize Cluster\nBased Routing Protocol (Cross-CBRP). Using NS-2 network simulator we evaluate\nrate of cluster head changes, throughput and packet delivery ratio. Comparisons\ndenote that Cross-CBRP has better performances with respect to the original\nCBRP.\n",
        "submitter": "Kazem Jahanbakhsh",
        "authors": "Kazem Jahanbakhsh, Marzieh Hajhosseini",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2008-02-06",
        "authors_parsed": "Jahanbakhsh Kazem; Hajhosseini Marzieh"
    },
    {
        "id": "803.4026",
        "title": "High-dimensional analysis of semidefinite relaxations for sparse\n  principal components",
        "abstract": "  Principal component analysis (PCA) is a classical method for dimensionality\nreduction based on extracting the dominant eigenvectors of the sample\ncovariance matrix. However, PCA is well known to behave poorly in the ``large\n$p$, small $n$'' setting, in which the problem dimension $p$ is comparable to\nor larger than the sample size $n$. This paper studies PCA in this\nhigh-dimensional regime, but under the additional assumption that the maximal\neigenvector is sparse, say, with at most $k$ nonzero components. We consider a\nspiked covariance model in which a base matrix is perturbed by adding a\n$k$-sparse maximal eigenvector, and we analyze two computationally tractable\nmethods for recovering the support set of this maximal eigenvector, as follows:\n(a) a simple diagonal thresholding method, which transitions from success to\nfailure as a function of the rescaled sample size\n$\\theta_{\\mathrm{dia}}(n,p,k)=n/[k^2\\log(p-k)]$; and (b) a more sophisticated\nsemidefinite programming (SDP) relaxation, which succeeds once the rescaled\nsample size $\\theta_{\\mathrm{sdp}}(n,p,k)=n/[k\\log(p-k)]$ is larger than a\ncritical threshold. In addition, we prove that no method, including the best\nmethod which has exponential-time complexity, can succeed in recovering the\nsupport if the order parameter $\\theta_{\\mathrm{sdp}}(n,p,k)$ is below a\nthreshold. Our results thus highlight an interesting trade-off between\ncomputational and statistical efficiency in high-dimensional inference.\n",
        "submitter": "Martin Wainwright",
        "authors": "Arash A. Amini, Martin J. Wainwright",
        "comments": "Published in at http://dx.doi.org/10.1214/08-AOS664 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Statistics 2009, Vol. 37, No. 5B, 2877-2921",
        "doi": "10.1214/08-AOS664",
        "report-no": "IMS-AOS-AOS664",
        "categories": "math.ST cs.IT math.IT stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-08-26",
        "authors_parsed": "Amini Arash A.; Wainwright Martin J."
    },
    {
        "id": "808.0309",
        "title": "A Reliable SVD based Watermarking Schem",
        "abstract": "  We propose a novel scheme for watermarking of digital images based on\nsingular value decomposition (SVD), which makes use of the fact that the SVD\nsubspace preserves significant amount of information of an image, as compared\nto its singular value matrix, Zhang and Li (2005). The principal components of\nthe watermark are embedded in the original image, leaving the detector with a\ncomplimentary set of singular vectors for watermark extraction. The above step\ninvariably ensures that watermark extraction from the embedded watermark image,\nusing a modified matrix, is not possible, thereby removing a major drawback of\nan earlier proposed algorithm by Liu and Tan (2002).\n",
        "submitter": "Siddharth Arora Mr.",
        "authors": "Chirag Jain, Siddharth Arora, and Prasanta K. Panigrahi",
        "comments": "8 Pages, 7 Figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.MM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2008-08-05",
        "authors_parsed": "Jain Chirag; Arora Siddharth; Panigrahi Prasanta K."
    },
    {
        "id": "812.0382",
        "title": "k-means requires exponentially many iterations even in the plane",
        "abstract": "  The k-means algorithm is a well-known method for partitioning n points that\nlie in the d-dimensional space into k clusters. Its main features are\nsimplicity and speed in practice. Theoretically, however, the best known upper\nbound on its running time (i.e. O(n^{kd})) can be exponential in the number of\npoints. Recently, Arthur and Vassilvitskii [3] showed a super-polynomial\nworst-case analysis, improving the best known lower bound from \\Omega(n) to\n2^{\\Omega(\\sqrt{n})} with a construction in d=\\Omega(\\sqrt{n}) dimensions. In\n[3] they also conjectured the existence of superpolynomial lower bounds for any\nd >= 2.\n  Our contribution is twofold: we prove this conjecture and we improve the\nlower bound, by presenting a simple construction in the plane that leads to the\nexponential lower bound 2^{\\Omega(n)}.\n",
        "submitter": "Andrea Vattani",
        "authors": "Andrea Vattani",
        "comments": "Submitted to SoCG 2009",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.CG cs.DS cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2008-12-03",
        "authors_parsed": "Vattani Andrea"
    },
    {
        "id": "901.4934",
        "title": "A historical perspective on developing foundations iInfo(TM) information\n  systems: iConsult(TM) and iEntertain(TM) apps using iDescribers(TM)\n  information integration for iOrgs(TM) information systems",
        "abstract": "  Technology now at hand can integrate all kinds of digital information for\nindividuals, groups, and organizations so their information usefully links\ntogether. iInfo(TM) information integration works by making connections\nincluding examples like the following:\n  - A statistical connection between \"being in a traffic jam\" and \"driving in\ndowntown Trenton between 5PM and 6PM on a weekday.\"\n  - A terminological connection between \"MSR\" and \"Microsoft Research.\"\n  - A causal connection between \"joining a group\" and \"being a member of the\ngroup.\"\n  - A syntactic connection between \"a pin dropped\" and \"a dropped pin.\"\n  - A biological connection between \"a dolphin\" and \"a mammal\".\n  - A demographic connection between \"undocumented residents of California\" and\n\"7% of the population of California.\"\n  - A geographical connection between \"Leeds\" and \"England.\"\n  - A temporal connection between \"turning on a computer\" and \"joining an\non-line discussion.\"\n  By making these connections, iInfo offers tremendous value for individuals,\nfamilies, groups, and organizations in making more effective use of information\ntechnology.\n  In practice, integrated information is invariably pervasively inconsistent.\nTherefore iInfo must be able to make connections even in the face of\ninconsistency. The business of iInfo is not to make difficult decisions like\ndeciding the ultimate truth or probability of propositions. Instead it provides\nmeans for processing information and carefully recording its provenance\nincluding arguments (including arguments about arguments) for and against\npropositions that is used by iConsult(TM) and iEntertain(TM) apps in iOrgs(TM)\nInformation Systems.\n  A historical perspective on the above questions is highly pertinent to the\ncurrent quest to develop foundations for privacy-friendly client-cloud\ncomputing.\n",
        "submitter": "Carl Hewitt",
        "authors": "Carl Hewitt",
        "comments": "updated title and abstract",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.DC cs.DB cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2010-10-06",
        "authors_parsed": "Hewitt Carl"
    },
    {
        "id": "902.4417",
        "title": "A Dimension Reduction Method for Inferring Biochemical Networks",
        "abstract": "  We present herein an extension of an algebraic statistical method for\ninferring biochemical reaction networks from experimental data, proposed\nrecently in [3]. This extension allows us to analyze reaction networks that are\nnot necessarily full-dimensional, i.e., the dimension of their stoichiometric\nspace is smaller than the number of species. Specifically, we propose to\naugment the original algebraic-statistical algorithm for network inference with\na preprocessing step that identifies the subspace spanned by the correct\nreaction vectors, within the space spanned by the species. This dimension\nreduction step is based on principal component analysis of the input data and\nits relationship with various subspaces generated by sets of candidate reaction\nvectors. Simulated examples are provided to illustrate the main ideas involved\nin implementing this method, and to asses its performance.\n",
        "submitter": "Grzegorz A Rempala",
        "authors": "Gheorghe Craciun, Casian Pantea, and Grzegorz A. Rempala",
        "comments": "12 pages and 4 figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": "MCG BBCB Tech Report No 1-09",
        "categories": "q-bio.MN q-bio.QM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-02-26",
        "authors_parsed": "Craciun Gheorghe; Pantea Casian; Rempala Grzegorz A."
    },
    {
        "id": "907.09",
        "title": "Impact of hierarchical modular structure on ranking of individual nodes\n  in directed networks",
        "abstract": "  Many systems, ranging from biological and engineering systems to social\nsystems, can be modeled as directed networks, with links representing directed\ninteraction between two nodes. To assess the importance of a node in a directed\nnetwork, various centrality measures based on different criteria have been\nproposed. However, calculating the centrality of a node is often difficult\nbecause of the overwhelming size of the network or the incomplete information\nabout the network. Thus, developing an approximation method for estimating\ncentrality measures is needed. In this study, we focus on modular networks;\nmany real-world networks are composed of modules, where connection is dense\nwithin a module and sparse across different modules. We show that ranking-type\ncentrality measures including the PageRank can be efficiently estimated once\nthe modular structure of a network is extracted. We develop an analytical\nmethod to evaluate the centrality of nodes by combining the local property\n(i.e., indegree and outdegree of nodes) and the global property (i.e.,\ncentrality of modules). The proposed method is corroborated with real data. Our\nresults provide a linkage between the ranking-type centrality values of modules\nand those of individual nodes. They also reveal the hierarchical structure of\nnetworks in the sense of subordination (not nestedness) laid out by\nconnectivity among modules of different relative importance. The present study\nraises a novel motive of identifying modules in networks.\n",
        "submitter": "Naoki Masuda Dr.",
        "authors": "Naoki Masuda, Yoji Kawamura, Hiroshi Kori",
        "comments": "4 figures, 4 tables. In v3, a major update has been done",
        "journal-ref": "New Journal of Physics, 11, 113002 (2009)",
        "doi": "10.1088/1367-2630/11/11/113002",
        "report-no": NaN,
        "categories": "physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-11-06",
        "authors_parsed": "Masuda Naoki; Kawamura Yoji; Kori Hiroshi"
    },
    {
        "id": "907.5477",
        "title": "A Nonlinear Approach to Dimension Reduction",
        "abstract": "  The $l_2$ flattening lemma of Johnson and Lindenstrauss [JL84] is a powerful\ntool for dimension reduction. It has been conjectured that the target dimension\nbounds can be refined and bounded in terms of the intrinsic dimensionality of\nthe data set (for example, the doubling dimension). One such problem was\nproposed by Lang and Plaut [LP01] (see also\n[GKL03,MatousekProblems07,ABN08,CGT10]), and is still open. We prove another\nresult in this line of work:\n  The snowflake metric $d^{1/2}$ of a doubling set $S \\subset l_2$ embeds with\nconstant distortion into $l_2^D$, for dimension $D$ that depends solely on the\ndoubling constant of the metric. In fact, the distortion can be made\narbitrarily close to 1, and the target dimension is polylogarithmic in the\ndoubling constant. Our techniques are robust and extend to the more difficult\nspaces $l_1$ and $l_\\infty$, although the dimension bounds here are\nquantitatively inferior than those for $l_2$.\n",
        "submitter": "Lee-Ad Gottlieb",
        "authors": "Lee-Ad Gottlieb, and Robert Krauthgamer",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": "10.1007/s00454-015-9707-9",
        "report-no": NaN,
        "categories": "cs.CG cs.DS math.FA math.MG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-06-09",
        "authors_parsed": "Gottlieb Lee-Ad; Krauthgamer Robert"
    },
    {
        "id": "909.3911",
        "title": "A Method for Extraction and Recognition of Isolated License Plate\n  Characters",
        "abstract": "  A method to extract and recognize isolated characters in license plates is\nproposed. In extraction stage, the proposed method detects isolated characters\nby using Difference-of-Gaussian (DOG) function, The DOG function, similar to\nLaplacian of Gaussian function, was proven to produce the most stable image\nfeatures compared to a range of other possible image functions. The candidate\ncharacters are extracted by doing connected component analysis on different\nscale DOG images. In recognition stage, a novel feature vector named\naccumulated gradient projection vector (AGPV) is used to compare the candidate\ncharacter with the standard ones. The AGPV is calculated by first projecting\npixels of similar gradient orientations onto specific axes, and then\naccumulates the projected gradient magnitudes by each axis. In the experiments,\nthe AGPVs are proven to be invariant from image scaling and rotation, and\nrobust to noise and illumination change.\n",
        "submitter": "R Doomun",
        "authors": "Yon Ping Chen, and Tien Der Yeh",
        "comments": "10 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/",
        "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 5, No. 1, pp. 1-10, August 2009, USA",
        "doi": NaN,
        "report-no": "ISSN 1947 5500",
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-09-23",
        "authors_parsed": "Chen Yon Ping; Der Yeh Tien"
    },
    {
        "id": "910.5449",
        "title": "Straight to the Source: Detecting Aggregate Objects in Astronomical\n  Images with Proper Error Control",
        "abstract": "  The next generation of telescopes will acquire terabytes of image data on a\nnightly basis. Collectively, these large images will contain billions of\ninteresting objects, which astronomers call sources. The astronomers' task is\nto construct a catalog detailing the coordinates and other properties of the\nsources. The source catalog is the primary data product for most telescopes and\nis an important input for testing new astrophysical theories, but to construct\nthe catalog one must first detect the sources. Existing algorithms for catalog\ncreation are effective at detecting sources, but do not have rigorous\nstatistical error control. At the same time, there are several multiple testing\nprocedures that provide rigorous error control, but they are not designed to\ndetect sources that are aggregated over several pixels. In this paper, we\npropose a technique that does both, by providing rigorous statistical error\ncontrol on the aggregate objects themselves rather than the pixels. We\ndemonstrate the effectiveness of this approach on data from the Chandra X-ray\nObservatory Satellite. Our technique effectively controls the rate of false\nsources, yet still detects almost all of the sources detected by procedures\nthat do not have such rigorous error control and have the advantage of\nadditional data in the form of follow up observations, which will not be\navailable for upcoming large telescopes. In fact, we even detect a new source\nthat was missed by previous studies. The statistical methods developed in this\npaper can be extended to problems beyond Astronomy, as we will illustrate with\nan example from Neuroimaging.\n",
        "submitter": "David Friedenberg",
        "authors": "David A. Friedenberg and Christopher R. Genovese",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.AP astro-ph.IM stat.ME",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-10-29",
        "authors_parsed": "Friedenberg David A.; Genovese Christopher R."
    },
    {
        "id": "911.1057",
        "title": "A Global Map of Science Based on the ISI Subject Categories",
        "abstract": "  The ISI subject categories classify journals included in the Science Citation\nIndex (SCI). The aggregated journal-journal citation matrix contained in the\nJournal Citation Reports can be aggregated on the basis of these categories.\nThis leads to an asymmetrical transaction matrix (citing versus cited) which is\nmuch more densely populated than the underlying matrix at the journal level.\nExploratory factor analysis leads us to opt for a fourteen-factor solution.\nThis solution can easily be interpreted as the disciplinary structure of\nscience. The nested maps of science (corresponding to 14 factors, 172\ncategories, and 6,164 journals) are brought online at\nhttp://www.leydesdorff.net/map06/index.htm. An analysis of interdisciplinary\nrelations is pursued at three levels of aggregation using the newly added ISI\nsubject category of \"Nanoscience & nanotechnology\". The journal level provides\nthe finer grained perspective. Errors in the attribution of journals to the ISI\nsubject categories are averaged out so that the factor analysis can reveal the\nmain structures. The mapping of science can, therefore, be comprehensive at the\nlevel of ISI subject categories.\n",
        "submitter": "Loet Leydesdorff",
        "authors": "Loet Leydesdorff, Ismael Rafols",
        "comments": NaN,
        "journal-ref": "Loet Leydesdorff & Ismael Rafols, A Global Map of Science Based on\n  the ISI Subject Categories, Journal of the American Society for Information\n  Science and Technology 60(2) (2009) 348-362",
        "doi": NaN,
        "report-no": NaN,
        "categories": "physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2009-11-06",
        "authors_parsed": "Leydesdorff Loet; Rafols Ismael"
    },
    {
        "id": "1001.0629",
        "title": "A New Algorithm for Multicommodity Flow",
        "abstract": "  We propose a new algorithm to obtain max flow for the multicommodity flow.\nThis algorithm utilizes the max-flow min-cut theorem and the well known\nlabeling algorithm due to Ford and Fulkerson [1]. We proceed as follows: We\nselect one source/sink pair among the n distinguished source/sink pairs at a\ntime and treat the given multicommodity network as a single commodity network\nfor such chosen source/sink pair. Then applying standard labeling algorithm,\nseparately for each sink/source pair, the feasible flow which is max flow and\nthe corresponding minimum cut corresponding to each source/sink pair is\nobtained. A record is made of these cuts and the paths flowing through the\nedges of these cuts. This record is then utilized to develop our algorithm to\nobtain max flow for multicommodity flow. In this paper we have pinpointed the\ndifficulty behind not getting a max flow min cut type theorem for\nmulticommodity flow and found out a remedy.\n",
        "submitter": "Dhananjay Mehendale",
        "authors": "Dhananjay P. Mehendale",
        "comments": "11 pages. Typos are corrected",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "math.GM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2010-01-13",
        "authors_parsed": "Mehendale Dhananjay P."
    },
    {
        "id": "1002.4727",
        "title": "Performance Analysis of Uplink & Downlink Transmission in CDMA System",
        "abstract": "  CDMA is a multiple access method in which the user's uses spread spectrum\ntechniques and occupy the entire spectrum whenever they transmit. In wireless\ncommunication signal-to-noise ratio (SNR) is the very important parameter that\ninfluences the system performance. Any mode of mobile transmission is not free\nfrom channel impairment such as noise, interference and fading. This channel\nimpairment caused signal distortion and degradation in SNR.Also there are\ndifferences between uplink (forward channel) and downlink (reverse\nchannel).Along with these differences, both the links use different codes for\nchanellizing the individual users. This paper simulates the expressions for the\npdfs of the SNR for both uplink and downlink transmission assuming that the\nsystem is operating at an average signal-to-noise ratio is 6dB per information\nbit.\n",
        "submitter": "Ashley Smith",
        "authors": "Md. M. Hossain, Md. M. Rahman and Md. A. Alim",
        "comments": "Journal of Telecommunications,Volume 1, Issue 1, pp84-86, February\n  2010",
        "journal-ref": "M. M. Hossain, M. M. Rahmand and M. A. Alim, \"Performance Analysis\n  of Uplink & Downlink Transmission in CDMA System\", Journal of\n  Telecommunications, Volume 1, Issue 1, pp84-86, February 2010",
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2010-02-26",
        "authors_parsed": "Hossain Md. M.; Rahman Md. M.; Alim Md. A."
    },
    {
        "id": "1010.4007",
        "title": "Colour Guided Colour Image Steganography",
        "abstract": "  Information security has become a cause of concern because of the electronic\neavesdropping. Capacity, robustness and invisibility are important parameters\nin information hiding and are quite difficult to achieve in a single algorithm.\nThis paper proposes a novel steganography technique for digital color image\nwhich achieves the purported targets. The professed methodology employs a\ncomplete random scheme for pixel selection and embedding of data. Of the three\ncolour channels (Red, Green, Blue) in a given colour image, the least two\nsignificant bits of any one of the channels of the color image is used to\nchannelize the embedding capacity of the remaining two channels. We have\ndevised three approaches to achieve various levels of our desired targets. In\nthe first approach, Red is the default guide but it results in localization of\nMSE in the remaining two channels, which makes it slightly vulnerable. In the\nsecond approach, user gets the liberty to select the guiding channel (Red,\nGreen or Blue) to guide the remaining two channels. It will increase the\nrobustness and imperceptibility of the embedded image however the MSE factor\nwill still remain as a drawback. The third approach improves the performance\nfactor as a cyclic methodology is employed and the guiding channel is selected\nin a cyclic fashion. This ensures the uniform distribution of MSE, which gives\nbetter robustness and imperceptibility along with enhanced embedding capacity.\nThe imperceptibility has been enhanced by suitably adapting optimal pixel\nadjustment process (OPAP) on the stego covers.\n",
        "submitter": "Gorge Sarah Prof.",
        "authors": "R.Amirtharajan, Sandeep Kumar Behera, Motamarri Abhilash Swarup,\n  Mohamed Ashfaaq K and John Bosco Balaguru Rayappan",
        "comments": "Universal Journal of Computer Science and Engineering Technology\n  (UniCSE)",
        "journal-ref": "1 (1), 16-23, Oct. 2010",
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.MM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2010-10-20",
        "authors_parsed": "Amirtharajan R.; Behera Sandeep Kumar; Swarup Motamarri Abhilash; K Mohamed Ashfaaq; Rayappan John Bosco Balaguru"
    },
    {
        "id": "1011.355",
        "title": "Overlay Protection Against Link Failures Using Network Coding",
        "abstract": "  This paper introduces a network coding-based protection scheme against single\nand multiple link failures. The proposed strategy ensures that in a connection,\neach node receives two copies of the same data unit: one copy on the working\ncircuit, and a second copy that can be extracted from linear combinations of\ndata units transmitted on a shared protection path. This guarantees\ninstantaneous recovery of data units upon the failure of a working circuit. The\nstrategy can be implemented at an overlay layer, which makes its deployment\nsimple and scalable. While the proposed strategy is similar in spirit to the\nwork of Kamal '07 & '10, there are significant differences. In particular, it\nprovides protection against multiple link failures. The new scheme is simpler,\nless expensive, and does not require the synchronization required by the\noriginal scheme. The sharing of the protection circuit by a number of\nconnections is the key to the reduction of the cost of protection. The paper\nalso conducts a comparison of the cost of the proposed scheme to the 1+1 and\nshared backup path protection (SBPP) strategies, and establishes the benefits\nof our strategy.\n",
        "submitter": "Shizheng Li",
        "authors": "Ahmed E. Kamal, Aditya Ramamoorthy, Long Long, Shizheng Li",
        "comments": "14 pages, 10 figures, accepted by IEEE/ACM Transactions on Networking",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2010-11-17",
        "authors_parsed": "Kamal Ahmed E.; Ramamoorthy Aditya; Long Long; Li Shizheng"
    },
    {
        "id": "1101.3594",
        "title": "Classification under Data Contamination with Application to Remote\n  Sensing Image Mis-registration",
        "abstract": "  This work is motivated by the problem of image mis-registration in remote\nsensing and we are interested in determining the resulting loss in the accuracy\nof pattern classification. A statistical formulation is given where we propose\nto use data contamination to model and understand the phenomenon of image\nmis-registration. This model is widely applicable to many other types of errors\nas well, for example, measurement errors and gross errors etc. The impact of\ndata contamination on classification is studied under a statistical learning\ntheoretical framework. A closed-form asymptotic bound is established for the\nresulting loss in classification accuracy, which is less than\n$\\epsilon/(1-\\epsilon)$ for data contamination of an amount of $\\epsilon$. Our\nbound is sharper than similar bounds in the domain adaptation literature and,\nunlike such bounds, it applies to classifiers with an infinite\nVapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on\nboth synthetic and real datasets under various types of data contamination,\nincluding label flipping, feature swapping and the replacement of feature\nvalues with data generated from a random source such as a Gaussian or Cauchy\ndistribution. Our simulation results show that the bound we derive is fairly\ntight.\n",
        "submitter": "Donghui Yan",
        "authors": "Donghui Yan, Peng Gong, Aiyou Chen and Liheng Zhong",
        "comments": "23 pages, 10 figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ME cs.LG stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-03-17",
        "authors_parsed": "Yan Donghui; Gong Peng; Chen Aiyou; Zhong Liheng"
    },
    {
        "id": "1102.0817",
        "title": "Natural images from the birthplace of the human eye",
        "abstract": "  Here we introduce a database of calibrated natural images publicly available\nthrough an easy-to-use web interface. Using a Nikon D70 digital SLR camera, we\nacquired about 5000 six-megapixel images of Okavango Delta of Botswana, a\ntropical savanna habitat similar to where the human eye is thought to have\nevolved. Some sequences of images were captured unsystematically while\nfollowing a baboon troop, while others were designed to vary a single parameter\nsuch as aperture, object distance, time of day or position on the horizon.\nImages are available in the raw RGB format and in grayscale. Images are also\navailable in units relevant to the physiology of human cone photoreceptors,\nwhere pixel values represent the expected number of photoisomerizations per\nsecond for cones sensitive to long (L), medium (M) and short (S) wavelengths.\nThis database is distributed under a Creative Commons Attribution-Noncommercial\nUnported license to facilitate research in computer vision, psychophysics of\nperception, and visual neuroscience.\n",
        "submitter": "Gasper Tkacik",
        "authors": "Ga\\v{s}per Tka\\v{c}ik and Patrick Garrigan and Charles Ratliff and\n  Grega Mil\\v{c}inski and Jennifer M Klein and Lucia H Seyfarth and Peter\n  Sterling and David Brainard and Vijay Balasubramanian",
        "comments": "Submitted to PLoS ONE",
        "journal-ref": "PLoS ONE 6: e20409 (2011)",
        "doi": "10.1371/journal.pone.0020409",
        "report-no": NaN,
        "categories": "q-bio.NC cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-08-01",
        "authors_parsed": "Tka\u010dik Ga\u0161per; Garrigan Patrick; Ratliff Charles; Mil\u010dinski Grega; Klein Jennifer M; Seyfarth Lucia H; Sterling Peter; Brainard David; Balasubramanian Vijay"
    },
    {
        "id": "1104.5186",
        "title": "Finding Dense Clusters via \"Low Rank + Sparse\" Decomposition",
        "abstract": "  Finding \"densely connected clusters\" in a graph is in general an important\nand well studied problem in the literature \\cite{Schaeffer}. It has various\napplications in pattern recognition, social networking and data mining\n\\cite{Duda,Mishra}. Recently, Ames and Vavasis have suggested a novel method\nfor finding cliques in a graph by using convex optimization over the adjacency\nmatrix of the graph \\cite{Ames, Ames2}. Also, there has been recent advances in\ndecomposing a given matrix into its \"low rank\" and \"sparse\" components\n\\cite{Candes, Chandra}. In this paper, inspired by these results, we view\n\"densely connected clusters\" as imperfect cliques, where imperfections\ncorrespond missing edges, which are relatively sparse. We analyze the problem\nin a probabilistic setting and aim to detect disjointly planted clusters. Our\nmain result basically suggests that, one can find \\emph{dense} clusters in a\ngraph, as long as the clusters are sufficiently large. We conclude by\ndiscussing possible extensions and future research directions.\n",
        "submitter": "Samet Oymak",
        "authors": "Samet Oymak, Babak Hassibi",
        "comments": "19 pages, 2 figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ML cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2011-04-28",
        "authors_parsed": "Oymak Samet; Hassibi Babak"
    },
    {
        "id": "1109.5454",
        "title": "The ubiquity of small-world networks",
        "abstract": "  Small-world networks by Watts and Strogatz are a class of networks that are\nhighly clustered, like regular lattices, yet have small characteristic path\nlengths, like random graphs. These characteristics result in networks with\nunique properties of regional specialization with efficient information\ntransfer. Social networks are intuitive examples of this organization with\ncliques or clusters of friends being interconnected, but each person is really\nonly 5-6 people away from anyone else. While this qualitative definition has\nprevailed in network science theory, in application, the standard quantitative\napplication is to compare path length (a surrogate measure of distributed\nprocessing) and clustering (a surrogate measure of regional specialization) to\nan equivalent random network. It is demonstrated here that comparing network\nclustering to that of a random network can result in aberrant findings and\nnetworks once thought to exhibit small-world properties may not. We propose a\nnew small-world metric, {\\omega} (omega), which compares network clustering to\nan equivalent lattice network and path length to a random network, as Watts and\nStrogatz originally described. Example networks are presented that would be\ninterpreted as small-world when clustering is compared to a random network but\nare not small-world according to {\\omega}. These findings have significant\nimplications in network science as small-world networks have unique topological\nproperties, and it is critical to accurately distinguish them from networks\nwithout simultaneous high clustering and low path length.\n",
        "submitter": "Qawi Telesford",
        "authors": "Qawi K. Telesford, Karen E. Joyce, Satoru Hayasaka, Jonathan H.\n  Burdette, Paul J. Laurienti",
        "comments": "29 pages, 8 figures, 2 tables",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "nlin.AO cs.SI physics.soc-ph",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "update_date": "2011-09-27",
        "authors_parsed": "Telesford Qawi K.; Joyce Karen E.; Hayasaka Satoru; Burdette Jonathan H.; Laurienti Paul J."
    },
    {
        "id": "1111.3689",
        "title": "CBLOCK: An Automatic Blocking Mechanism for Large-Scale De-duplication\n  Tasks",
        "abstract": "  De-duplication---identification of distinct records referring to the same\nreal-world entity---is a well-known challenge in data integration. Since very\nlarge datasets prohibit the comparison of every pair of records, {\\em blocking}\nhas been identified as a technique of dividing the dataset for pairwise\ncomparisons, thereby trading off {\\em recall} of identified duplicates for {\\em\nefficiency}. Traditional de-duplication tasks, while challenging, typically\ninvolved a fixed schema such as Census data or medical records. However, with\nthe presence of large, diverse sets of structured data on the web and the need\nto organize it effectively on content portals, de-duplication systems need to\nscale in a new dimension to handle a large number of schemas, tasks and data\nsets, while handling ever larger problem sizes. In addition, when working in a\nmap-reduce framework it is important that canopy formation be implemented as a\n{\\em hash function}, making the canopy design problem more challenging. We\npresent CBLOCK, a system that addresses these challenges. CBLOCK learns hash\nfunctions automatically from attribute domains and a labeled dataset consisting\nof duplicates. Subsequently, CBLOCK expresses blocking functions using a\nhierarchical tree structure composed of atomic hash functions. The application\nmay guide the automated blocking process based on architectural constraints,\nsuch as by specifying a maximum size of each block (based on memory\nrequirements), impose disjointness of blocks (in a grid environment), or\nspecify a particular objective function trading off recall for efficiency. As a\npost-processing step to automatically generated blocks, CBLOCK {\\em rolls-up}\nsmaller blocks to increase recall. We present experimental results on two\nlarge-scale de-duplication datasets at Yahoo!---consisting of over 140K movies\nand 40K restaurants respectively---and demonstrate the utility of CBLOCK.\n",
        "submitter": "Anish Das Sarma",
        "authors": "Anish Das Sarma, Ankur Jain, Ashwin Machanavajjhala, Philip Bohannon",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2011-11-17",
        "authors_parsed": "Sarma Anish Das; Jain Ankur; Machanavajjhala Ashwin; Bohannon Philip"
    },
    {
        "id": "1112.0243",
        "title": "W. W. Morgan and the Discovery of the Spiral Arm Structure of our Galaxy",
        "abstract": "  William Wilson Morgan was one of the great astronomers of the twentieth\ncentury. He considered himself a morphologist, and was preoccupied throughout\nhis career with matters of classification. Though his early life was difficult,\nand his pursuit of astronomy as a career was opposed by his father, he took a\nposition at Yerkes Observatory in 1926 and remained there for the rest of his\nworking life. Thematically, his work was also a unified whole. Beginning with\nspectroscopic studies under Otto Struve at Yerkes Observatory, by the late\n1930s he concentrated particularly on the young O and B stars. His work on\nstellar classification led to the Morgan-Keenan- Kellman [MKK] system of\nclassification of stars, and later - as he grappled with the question of the\nintrinsic color and brightness of stars at great distances - to the\nJohnson-Morgan UBV system for measuring stellar colors. Eventually these\nconcerns with classification and method led to his greatest single achievement\n- the recognition of the nearby spiral arms of our Galaxy by tracing the OB\nassociations and HII regions that outline them. After years of intensive work\non the problem of galactic structure, the discovery came in a blinding flash of\nArchimedean insight as he walked under the night sky between his office and his\nhouse in the autumn of 1951. His optical discovery of the spiral arms preceded\nthe radio-mapping of the spiral arms by more than a year. Morgan suffered a\nnervous breakdown soon after he announced his discovery, however, and so was\nprevented from publishing a complete account of his work. As a result of that,\nand the announcement soon afterward of the first radio maps of the spiral arms,\nthe uniqueness of his achievement was not fully appreciated at the time.\n",
        "submitter": "Curtis Struck",
        "authors": "William Sheehan",
        "comments": "21 pages in Journal of Astronomical History and Heritage",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "physics.hist-ph astro-ph.GA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2011-12-02",
        "authors_parsed": "Sheehan William"
    },
    {
        "id": "1112.1688",
        "title": "Why don't we already have an Integrated Framework for the Publication\n  and Preservation of all Data Products?",
        "abstract": "  Astronomy has long had a working network of archives supporting the curation\nof publications and data. The discipline has already created many of the\nfeatures which perplex other areas of science: (1) data repositories:\n(supra)national institutes, dedicated to large projects; a culture of\nuser-contributed data; practical experience of long-term data preservation; (2)\ndataset identifiers: the community has already piloted experiments, knows what\ncan undermine these efforts, and is participating in the development of\nnext-generation standards; (3) citation of datasets in papers: the community\nhas an innovative and expanding infrastructure for the curation of data and\nbibliographic resources, and through them a community of author s and editors\nfamiliar with such electronic publication efforts; as well, it has experimented\nwith next-generation web standards (e.g. the Semantic Web); (4) publisher\nbuy-in: publishers in this area have been willing to innovate within the\nconstraints of their commercial imperatives. What can possibly be missing? Why\ndon't we have an integrated framework for the publication and preservation of\nall data products already? Are there technical barriers? We don't believe so.\nAre there cultural or commercial forces inhibiting this? We aren't aware of\nany. This Birds of a Feather session (BoF) attempted to identify existing\nbarriers to the creation of such a framework, and attempted to identify the\nparties or groups which can contribute to the creation of a VO-powered\ndata-publishing framework.\n",
        "submitter": "Alberto Accomazzi",
        "authors": "Alberto Accomazzi, Sebastien Derriere, Chris Biemesderfer and Norman\n  Gray",
        "comments": "4 pages, submitted to the ADASS XXI proceedings",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "astro-ph.IM cs.DL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2011-12-08",
        "authors_parsed": "Accomazzi Alberto; Derriere Sebastien; Biemesderfer Chris; Gray Norman"
    },
    {
        "id": "1201.1281",
        "title": "Astronomy with Cutting-Edge ICT: From Transients in the Sky to Data over\n  the Continents (India-US)",
        "abstract": "  Astronomy has always been at the forefront of information technology, moving\nfrom the era of photographic plates, to digital snapshots and now to digital\nmovies of the sky. This has brought about a data explosion with multi- terabyte\nsurveys already happening and upcoming petabyte scale surveys. By scanning the\nsky repeatedly and automatically, astronomers find rapidly changing phenomena -\ntransients - of a great variety. Surveys like the Catalina Real-time Transient\nSurvey (CRTS) publish details on the transients right away since many of these\nfade in a matter of minutes and it is important to get additional observations\nin order to determine their nature. This involves being able to combine a\nvariety of datasets, small and large, in real-time. With networks like the Asia\nPacific Advanced Network (APAN) and India's National Knowledge Network (NKN) we\nare in the realm where such a data transfer is possible in real time across\ncontinents. Here we describe the live demonstration we were able to carry out\nat data transfer speeds of several hundred megabits per second (Mbps) between\nCalifornia Institute of Technology (Caltech, USA) and the Inter-University\nCentre for Astronomy and Astrophysics (IUCAA, India). This project illustrates\nhow machines can make rapid decisions in response to complex, heterogeneous\ndata, using sophisticated software and networking. While the broader impact\ncovers all aspects of society (disaster response, power grids, earthquakes, and\nmany more), we have used astronomy to show how the APAN and NKN make this\npossible.\n",
        "submitter": "Ashish Mahabal",
        "authors": "Ashish Mahabal, Ajit Kembhavi, Roy Williams and Sharmad Navelkar",
        "comments": "PDF; 8 pages; includes 3 figures; To appear in Proceedings of the\n  32nd Asia-Pacific Advanced Network Meeting Ed. Chris Elvidge See\n  http://usymposia.upm.my/index.php/APAN_Proceedings/32nd_APAN/schedConf/presentations",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "astro-ph.IM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2012-01-06",
        "authors_parsed": "Mahabal Ashish; Kembhavi Ajit; Williams Roy; Navelkar Sharmad"
    },
    {
        "id": "1201.1507",
        "title": "Sampling properties of directed networks",
        "abstract": "  For many real-world networks only a small \"sampled\" version of the original\nnetwork may be investigated; those results are then used to draw conclusions\nabout the actual system. Variants of breadth-first search (BFS) sampling, which\nare based on epidemic processes, are widely used. Although it is well\nestablished that BFS sampling fails, in most cases, to capture the\nIN-component(s) of directed networks, a description of the effects of BFS\nsampling on other topological properties are all but absent from the\nliterature. To systematically study the effects of sampling biases on directed\nnetworks, we compare BFS sampling to random sampling on complete large-scale\ndirected networks. We present new results and a thorough analysis of the\ntopological properties of seven different complete directed networks (prior to\nsampling), including three versions of Wikipedia, three different sources of\nsampled World Wide Web data, and an Internet-based social network. We detail\nthe differences that sampling method and coverage can make to the structural\nproperties of sampled versions of these seven networks. Most notably, we find\nthat sampling method and coverage affect both the bow-tie structure, as well as\nthe number and structure of strongly connected components in sampled networks.\nIn addition, at low sampling coverage (i.e. less than 40%), the values of\naverage degree, variance of out-degree, degree auto-correlation, and link\nreciprocity are overestimated by 30% or more in BFS-sampled networks, and only\nattain values within 10% of the corresponding values in the complete networks\nwhen sampling coverage is in excess of 65%. These results may cause us to\nrethink what we know about the structure, function, and evolution of real-world\ndirected networks.\n",
        "submitter": "Seung-Woo Son",
        "authors": "Seung-Woo Son, Claire Christensen, Golnoosh Bizhani, David V. Foster,\n  Peter Grassberger, and Maya Paczuski",
        "comments": "21 pages, 11 figures",
        "journal-ref": "Phys. Rev. E 86, 046104 (2012)",
        "doi": "10.1103/PhysRevE.86.046104",
        "report-no": NaN,
        "categories": "physics.soc-ph cs.SI physics.data-an",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-03-19",
        "authors_parsed": "Son Seung-Woo; Christensen Claire; Bizhani Golnoosh; Foster David V.; Grassberger Peter; Paczuski Maya"
    },
    {
        "id": "1202.523",
        "title": "Triadic Measures on Graphs: The Power of Wedge Sampling",
        "abstract": "  Graphs are used to model interactions in a variety of contexts, and there is\na growing need to quickly assess the structure of a graph. Some of the most\nuseful graph metrics, especially those measuring social cohesion, are based on\ntriangles. Despite the importance of these triadic measures, associated\nalgorithms can be extremely expensive. We propose a new method based on wedge\nsampling. This versatile technique allows for the fast and accurate\napproximation of all current variants of clustering coefficients and enables\nrapid uniform sampling of the triangles of a graph. Our methods come with\nprovable and practical time-approximation tradeoffs for all computations. We\nprovide extensive results that show our methods are orders of magnitude faster\nthan the state-of-the-art, while providing nearly the accuracy of full\nenumeration. Our results will enable more wide-scale adoption of triadic\nmeasures for analysis of extremely large graphs, as demonstrated on several\nreal-world examples.\n",
        "submitter": "Tamara Kolda",
        "authors": "C. Seshadhri, Ali Pinar, Tamara G. Kolda",
        "comments": NaN,
        "journal-ref": "SDM13: Proceedings of the 2013 SIAM International Conference on\n  Data Mining, pp. 10-18, May 2013",
        "doi": "10.1137/1.9781611972832.2",
        "report-no": NaN,
        "categories": "cs.SI cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2014-04-22",
        "authors_parsed": "Seshadhri C.; Pinar Ali; Kolda Tamara G."
    },
    {
        "id": "1204.5592",
        "title": "Dynamic and Auto Responsive Solution for Distributed Denial-of-Service\n  Attacks Detection in ISP Network",
        "abstract": "  Denial of service (DoS) attacks and more particularly the distributed ones\n(DDoS) are one of the latest threat and pose a grave danger to users,\norganizations and infrastructures of the Internet. Several schemes have been\nproposed on how to detect some of these attacks, but they suffer from a range\nof problems, some of them being impractical and others not being effective\nagainst these attacks. This paper reports the design principles and evaluation\nresults of our proposed framework that autonomously detects and accurately\ncharacterizes a wide range of flooding DDoS attacks in ISP network. Attacks are\ndetected by the constant monitoring of propagation of abrupt traffic changes\ninside ISP network. For this, a newly designed flow-volume based approach\n(FVBA) is used to construct profile of the traffic normally seen in the\nnetwork, and identify anomalies whenever traffic goes out of profile.\nConsideration of varying tolerance factors make proposed detection system\nscalable to the varying network conditions and attack loads in real time.\nSix-sigma method is used to identify threshold values accurately for malicious\nflows characterization. FVBA has been extensively evaluated in a controlled\ntest-bed environment. Detection thresholds and efficiency is justified using\nreceiver operating characteristics (ROC) curve. For validation, KDD 99, a\npublicly available benchmark dataset is used. The results show that our\nproposed system gives a drastic improvement in terms of detection and false\nalarm rate.\n",
        "submitter": "Dr Brij Gupta",
        "authors": "B. B. Gupta, R. C. Joshi, Manoj Misra",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1203.2400",
        "journal-ref": "International Journal of Computer Theory and Engineering, Vol. 1,\n  No. 1, April 2009 1793-821X",
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2012-04-26",
        "authors_parsed": "Gupta B. B.; Joshi R. C.; Misra Manoj"
    },
    {
        "id": "1210.119",
        "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix\n  Factorization",
        "abstract": "  The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)\nturns non-negative matrix factorization (NMF) into a tractable problem.\nRecently, a new class of provably-correct NMF algorithms have emerged under\nthis assumption. In this paper, we reformulate the separable NMF problem as\nthat of finding the extreme rays of the conical hull of a finite set of\nvectors. From this geometric perspective, we derive new separable NMF\nalgorithms that are highly scalable and empirically noise robust, and have\nseveral other favorable properties in relation to existing methods. A parallel\nimplementation of our algorithm demonstrates high scalability on shared- and\ndistributed-memory machines.\n",
        "submitter": "Abhishek Kumar",
        "authors": "Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur",
        "comments": "15 pages, 6 figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ML cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2012-10-04",
        "authors_parsed": "Kumar Abhishek; Sindhwani Vikas; Kambadur Prabhanjan"
    },
    {
        "id": "1210.148",
        "title": "Theoretical And Technological Building Blocks For An Innovation\n  Accelerator",
        "abstract": "  The scientific system that we use today was devised centuries ago and is\ninadequate for our current ICT-based society: the peer review system encourages\nconservatism, journal publications are monolithic and slow, data is often not\navailable to other scientists, and the independent validation of results is\nlimited. Building on the Innovation Accelerator paper by Helbing and Balietti\n(2011) this paper takes the initial global vision and reviews the theoretical\nand technological building blocks that can be used for implementing an\ninnovation (in first place: science) accelerator platform driven by\nre-imagining the science system. The envisioned platform would rest on four\npillars: (i) Redesign the incentive scheme to reduce behavior such as\nconservatism, herding and hyping; (ii) Advance scientific publications by\nbreaking up the monolithic paper unit and introducing other building blocks\nsuch as data, tools, experiment workflows, resources; (iii) Use machine\nreadable semantics for publications, debate structures, provenance etc. in\norder to include the computer as a partner in the scientific process, and (iv)\nBuild an online platform for collaboration, including a network of trust and\nreputation among the different types of stakeholders in the scientific system:\nscientists, educators, funding agencies, policy makers, students and industrial\ninnovators among others. Any such improvements to the scientific system must\nsupport the entire scientific process (unlike current tools that chop up the\nscientific process into disconnected pieces), must facilitate and encourage\ncollaboration and interdisciplinarity (again unlike current tools), must\nfacilitate the inclusion of intelligent computing in the scientific process,\nmust facilitate not only the core scientific process, but also accommodate\nother stakeholders such science policy makers, industrial innovators, and the\ngeneral public.\n",
        "submitter": "George Kampis",
        "authors": "Frank van Harmelen and George Kampis and Katy Borner and Peter van den\n  Besselaar and Erik Schultes and Carole Goble and Paul Groth and Barend Mons\n  and Stuart Anderson and Stefan Decker and Conor Hayes and Thierry Buecheler\n  and Dirk Helbing",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": "10.1140/epjst/e2012-01692-1",
        "report-no": NaN,
        "categories": "cs.DL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-06-11",
        "authors_parsed": "van Harmelen Frank; Kampis George; Borner Katy; Besselaar Peter van den; Schultes Erik; Goble Carole; Groth Paul; Mons Barend; Anderson Stuart; Decker Stefan; Hayes Conor; Buecheler Thierry; Helbing Dirk"
    },
    {
        "id": "1210.7844",
        "title": "Unified spectral bounds on the chromatic number",
        "abstract": "  One of the best known results in spectral graph theory is the following lower\nbound on the chromatic number due to Alan Hoffman, where mu_1 and mu_n are\nrespectively the maximum and minimum eigenvalues of the adjacency matrix: chi\n>= 1 + mu_1 / (- mu_n). We recently generalised this bound to include all\neigenvalues of the adjacency matrix.\n  In this paper, we further generalize these results to include all eigenvalues\nof the adjacency, Laplacian and signless Laplacian matrices. The various known\nbounds are also unified by considering the normalized adjacency matrix, and\nexamples are cited for which the new bounds outperform known bounds.\n",
        "submitter": "Pawel Wocjan",
        "authors": "Clive Elphick and Pawel Wocjan",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "math.CO cs.DM quant-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2014-10-30",
        "authors_parsed": "Elphick Clive; Wocjan Pawel"
    },
    {
        "id": "1210.8267",
        "title": "Detecting Linear Block Codes in Noise using the GLRT",
        "abstract": "  In this paper, we consider the problem of distinguishing the noisy codewords\nof a known binary linear block code from a random bit sequence. We propose to\nuse the generalized likelihood ratio test (GLRT) to solve this problem. We also\ngive a formula to find approximate number of codewords required and compare our\nresults with an existing method.\n",
        "submitter": "Arti Yardi",
        "authors": "Arti D. Yardi, Saravanan Vijayakumaran",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2012-11-01",
        "authors_parsed": "Yardi Arti D.; Vijayakumaran Saravanan"
    },
    {
        "id": "1212.094",
        "title": "Computing the Image of the City",
        "abstract": "  Kevin Lynch proposed a theory of the image of the city identifying five\nelements that make the city legible or imageable. The resulting mental map of\nthe city was conventionally derived through some qualitative processes, relying\non interactions with city residents to ask them to recall city elements from\ntheir minds. This paper proposes a process by which the image of the city can\nbe quantitatively derived automatically using computer technology and\ngeospatial databases of the city. This method is substantially based on and\ninspired by Christopher Alexander's living structure and Nikos Salingaros'\nstructural order, as a city with the living structure or structural order tends\nto be legible and imageable. With the increasing availability of geographic\ninformation of urban environments at very fine scales or resolutions (for\nexample, trajectories data about human activities), the proposal or solution\ndescribed in this paper is particularly timely and relevant for urban studies\nand architectural design.\n  Keywords: Mental maps, head/tail division rule, legibility, imageability,\npower law, scaling, and hierarchy.\n",
        "submitter": "Bin Jiang",
        "authors": "Bin Jiang",
        "comments": "11 pages, 2 figures, and 1 table; In: Campagna M., De Montis A.,\n  Isola F., Lai S., Pira C. and Zoppi C. (editors, 2012), Planning Support\n  Tools: Policy analysis, implementation and evaluation, Proceedings of the 7th\n  Int. conf. on Informatics and Urban and Regional Planning INPUT 2012, 111-121",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": "Campagna M., De Montis A., Isola F., Lai S., Pira C. and Zoppi C.\n  (editors, 2012), Planning Support Tools: Policy analysis, implementation and\n  evaluation, Proceedings of the 7th Int. conf. on Informatics and Urban and\n  Regional Planning INPUT 2012, 111-121",
        "categories": "nlin.AO physics.soc-ph",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "update_date": "2015-03-13",
        "authors_parsed": "Jiang Bin"
    },
    {
        "id": "1212.4137",
        "title": "Alternating Maximization: Unifying Framework for 8 Sparse PCA\n  Formulations and Efficient Parallel Codes",
        "abstract": "  Given a multivariate data set, sparse principal component analysis (SPCA)\naims to extract several linear combinations of the variables that together\nexplain the variance in the data as much as possible, while controlling the\nnumber of nonzero loadings in these combinations. In this paper we consider 8\ndifferent optimization formulations for computing a single sparse loading\nvector; these are obtained by combining the following factors: we employ two\nnorms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),\nwhich are used in two different ways (constraint, penalty). Three of our\nformulations, notably the one with L0 constraint and L1 variance, have not been\nconsidered in the literature. We give a unifying reformulation which we propose\nto solve via a natural alternating maximization (AM) method. We show the the AM\nmethod is nontrivially equivalent to GPower (Journ\\'{e}e et al; JMLR\n11:517--553, 2010) for all our formulations. Besides this, we provide 24\nefficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)\nfor each of the 8 problems. Parallelism in the methods is aimed at i) speeding\nup computations (our GPU code can be 100 times faster than an efficient serial\ncode written in C++), ii) obtaining solutions explaining more variance and iii)\ndealing with big data problems (our cluster code is able to solve a 357 GB\nproblem in about a minute).\n",
        "submitter": "Martin Tak\\'a\\v{c}",
        "authors": "Peter Richt\\'arik, Majid Jahani, Selin Damla Ahipa\\c{s}ao\\u{g}lu,\n  Martin Tak\\'a\\v{c}",
        "comments": "29 pages, 9 tables, 7 figures (the paper is accompanied by a release\n  of the open-source code '24am')",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ML cs.LG math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2020-05-08",
        "authors_parsed": "Richt\u00e1rik Peter; Jahani Majid; Ahipa\u015fao\u011flu Selin Damla; Tak\u00e1\u010d Martin"
    },
    {
        "id": "1302.073",
        "title": "Measure Transformed Independent Component Analysis",
        "abstract": "  In this paper we derive a new framework for independent component analysis\n(ICA), called measure-transformed ICA (MTICA), that is based on applying a\nstructured transform to the probability distribution of the observation vector,\ni.e., transformation of the probability measure defined on its observation\nspace. By judicious choice of the transform we show that the separation matrix\ncan be uniquely determined via diagonalization of several measure-transformed\ncovariance matrices. In MTICA the separation matrix is estimated via\napproximate joint diagonalization of several empirical measure-transformed\ncovariance matrices. Unlike kernel based ICA techniques where the\ntransformation is applied repetitively to some affine mappings of the\nobservation vector, in MTICA the transformation is applied only once to the\nprobability distribution of the observations. This results in performance\nadvantages and reduced implementation complexity. Simulations demonstrate the\nadvantages of the proposed approach as compared to other existing\nstate-of-the-art methods for ICA.\n",
        "submitter": "Koby Todros",
        "authors": "Koby Todros and Alfred O. Hero",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ME",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-12-10",
        "authors_parsed": "Todros Koby; Hero Alfred O."
    },
    {
        "id": "1302.4465",
        "title": "Unveiling the relationship between complex networks metrics and word\n  senses",
        "abstract": "  The automatic disambiguation of word senses (i.e., the identification of\nwhich of the meanings is used in a given context for a word that has multiple\nmeanings) is essential for such applications as machine translation and\ninformation retrieval, and represents a key step for developing the so-called\nSemantic Web. Humans disambiguate words in a straightforward fashion, but this\ndoes not apply to computers. In this paper we address the problem of Word Sense\nDisambiguation (WSD) by treating texts as complex networks, and show that word\nsenses can be distinguished upon characterizing the local structure around\nambiguous words. Our goal was not to obtain the best possible disambiguation\nsystem, but we nevertheless found that in half of the cases our approach\noutperforms traditional shallow methods. We show that the hierarchical\nconnectivity and clustering of words are usually the most relevant features for\nWSD. The results reported here shine light on the relationship between semantic\nand structural parameters of complex networks. They also indicate that when\ncombined with traditional techniques the complex network approach may be useful\nto enhance the discrimination of senses in large texts\n",
        "submitter": "Diego  Amancio Raphael",
        "authors": "Diego R. Amancio, Osvaldo N. Oliveira Jr. and Luciano da F. Costa",
        "comments": "The Supplementary Information (SI) is available from\n  http://dl.dropbox.com/u/2740286/epl_SI.pdf",
        "journal-ref": "Europhysics Letters (2012) 98 18002",
        "doi": "10.1209/0295-5075/98/18002",
        "report-no": NaN,
        "categories": "physics.soc-ph cs.CL cs.SI physics.data-an",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-02-20",
        "authors_parsed": "Amancio Diego R.; Oliveira Osvaldo N. Jr.; Costa Luciano da F."
    },
    {
        "id": "1303.1738",
        "title": "Mixing local and global information for community detection in large\n  networks",
        "abstract": "  The problem of clustering large complex networks plays a key role in several\nscientific fields ranging from Biology to Sociology and Computer Science. Many\napproaches to clustering complex networks are based on the idea of maximizing a\nnetwork modularity function. Some of these approaches can be classified as\nglobal because they exploit knowledge about the whole network topology to find\nclusters. Other approaches, instead, can be interpreted as local because they\nrequire only a partial knowledge of the network topology, e.g., the neighbors\nof a vertex. Global approaches are able to achieve high values of modularity\nbut they do not scale well on large networks and, therefore, they cannot be\napplied to analyze on-line social networks like Facebook or YouTube. In\ncontrast, local approaches are fast and scale up to large, real-life networks,\nat the cost of poorer results than those achieved by local methods. In this\narticle we propose a glocal method to maximizing modularity, i.e., our method\nuses information at the global level, yet its scalability on large networks is\ncomparable to that of local methods. The proposed method is called COmplex\nNetwork CLUster DEtection (or, shortly, CONCLUDE.) It works in two stages: in\nthe first stage it uses an information-propagation model, based on random and\nnon-backtracking walks of finite length, to compute the importance of each edge\nin keeping the network connected (called edge centrality.) Then, edge\ncentrality is used to map network vertices onto points of an Euclidean space\nand to compute distances between all pairs of connected vertices. In the second\nstage, CONCLUDE uses the distances computed in the first stage to partition the\nnetwork into clusters. CONCLUDE is computationally efficient since in the\naverage case its cost is roughly linear in the number of edges of the network.\n",
        "submitter": "Emilio Ferrara",
        "authors": "Pasquale De Meo, Emilio Ferrara, Giacomo Fiumara, Alessandro Provetti",
        "comments": NaN,
        "journal-ref": "Journal of Computer and System Sciences 80(1):72-87, 2014",
        "doi": "10.1016/j.jcss.2013.03.012",
        "report-no": NaN,
        "categories": "cs.SI physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-10-17",
        "authors_parsed": "De Meo Pasquale; Ferrara Emilio; Fiumara Giacomo; Provetti Alessandro"
    },
    {
        "id": "1304.5022",
        "title": "Efficacy of Attack detection capability of IDPS based on it's deployment\n  in wired and wireless environment",
        "abstract": "  Intrusion Detection and/or Prevention Systems (IDPS) represent an important\nline of defence against a variety of attacks that can compromise the security\nand proper functioning of an enterprise information system. Along with the\nwidespread evolution of new emerging services, the quantity and impact of\nattacks have continuously increased, attackers continuously find\nvulnerabilities at various levels, from the network itself to operating system\nand applications, exploit them to crack system and services. Network defence\nand network monitoring has become an essential component of computer security\nto predict and prevent attacks. Unlike traditional Intrusion Detection System\n(IDS), Intrusion Detection and Prevention System (IDPS) have additional\nfeatures to secure computer networks. In this paper, we present a detailed\nstudy of how deployment of an IDPS plays a key role in its performance and the\nability to detect and prevent known as well as unknown attacks. We categorize\nIDPS based on deployment as Network-based, host-based, and Perimeter-based and\nHybrid. A detailed comparison is shown in this paper and finally we justify our\nproposed solution, which deploys agents at host-level to give better\nperformance in terms of reduced rate of false positives and accurate detection\nand prevention.\n",
        "submitter": "Shalvi Dave D",
        "authors": "Shalvi Dave, Bhushan Trivedi, Jimit Mahadevia",
        "comments": "13 pages, 10 figures",
        "journal-ref": "International Journal of Network Security & Its Applications\n  (IJNSA), Vol.5, No.2, March 2013",
        "doi": "10.5121/ijnsa.2013.5208",
        "report-no": NaN,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-04-19",
        "authors_parsed": "Dave Shalvi; Trivedi Bhushan; Mahadevia Jimit"
    },
    {
        "id": "1307.0475",
        "title": "A Random Matrix Approach to Differential Privacy and Structure Preserved\n  Social Network Graph Publishing",
        "abstract": "  Online social networks are being increasingly used for analyzing various\nsocietal phenomena such as epidemiology, information dissemination, marketing\nand sentiment flow. Popular analysis techniques such as clustering and\ninfluential node analysis, require the computation of eigenvectors of the real\ngraph's adjacency matrix. Recent de-anonymization attacks on Netflix and AOL\ndatasets show that an open access to such graphs pose privacy threats. Among\nthe various privacy preserving models, Differential privacy provides the\nstrongest privacy guarantees.\n  In this paper we propose a privacy preserving mechanism for publishing social\nnetwork graph data, which satisfies differential privacy guarantees by\nutilizing a combination of theory of random matrix and that of differential\nprivacy. The key idea is to project each row of an adjacency matrix to a low\ndimensional space using the random projection approach and then perturb the\nprojected matrix with random noise. We show that as compared to existing\napproaches for differential private approximation of eigenvectors, our approach\nis computationally efficient, preserves the utility and satisfies differential\nprivacy. We evaluate our approach on social network graphs of Facebook, Live\nJournal and Pokec. The results show that even for high values of noise variance\nsigma=1 the clustering quality given by normalized mutual information gain is\nas low as 0.74. For influential node discovery, the propose approach is able to\ncorrectly recover 80 of the most influential nodes. We also compare our results\nwith an approach presented in [43], which directly perturbs the eigenvector of\nthe original data by a Laplacian noise. The results show that this approach\nrequires a large random perturbation in order to preserve the differential\nprivacy, which leads to a poor estimation of eigenvectors for large social\nnetworks.\n",
        "submitter": "Faraz Ahmed",
        "authors": "Faraz Ahmed, Rong Jin and Alex X. Liu",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.CR cs.SI physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-07-02",
        "authors_parsed": "Ahmed Faraz; Jin Rong; Liu Alex X."
    },
    {
        "id": "1307.1834",
        "title": "Multiple Vectors Propagation of Epidemics in Complex Networks",
        "abstract": "  This letter investigates the epidemic spreading in two-vectors propagation\nnetwork (TPN). We propose detailed theoretical analysis that allows us to\naccurately calculate the epidemic threshold and outbreak size. It is found that\nthe epidemics can spread across the TPN even if two sub-single-vector\npropagation networks (SPNs) of TPN are well below their respective epidemic\nthresholds. Strong positive degree-degree correlation of nodes in TPN could\nlead to a much lower epidemic threshold and a relatively smaller outbreak size.\nHowever, the average similarity between the neighbors from different SPNs of\nnodes has no effect on the epidemic threshold and outbreak size.\n",
        "submitter": "Dawei Zhao",
        "authors": "Dawei Zhao, Lixiang Li, Haipeng Peng, Qun Luo, Yixian Yang",
        "comments": NaN,
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "physics.soc-ph cs.SI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-07-09",
        "authors_parsed": "Zhao Dawei; Li Lixiang; Peng Haipeng; Luo Qun; Yang Yixian"
    },
    {
        "id": "1307.7788",
        "title": "Computer Aided Investigation: Visualization and Analysis of data from\n  Mobile communication devices using Formal Concept Analysis",
        "abstract": "  In this world of terrorism, it is very important to know the network of\nindividual suspects. It is also important to analyze the attributes of members\nof a network and the relationships that exist between them either directly or\nindirectly. This will make it easy for concepts to be built in aiding criminal\ninvestigations. However traditional approaches cannot be used to visualize and\nanalyze data collected on individuals. With this current day where information\nsystems play critical role in everyday life of every individual, it is easier\nto depend on digital information in fighting crime.Effective computer tools and\nintelligent systems that are automated to analyze and interpret criminal data\nin real time effectively and efficiently are needed in fighting crime. These\ncurrent computer systems should have the capability of providing intelligence\nfrom raw data and creating a visual graph which will make it easy for new\nconcepts to be built and generated from crime data in order to solve understand\nand analyze crime patterns easily. This paper proposes a new method of computer\naided investigation by visualizing and analyzing data of mobile communication\ndevices using Formal Concept Analysis, or Galois Lattices, a data analysis\ntechnique grounded on Lattice Theory and Propositional Calculus. This method\nconsidered the set of common and distinct attributes of data in such a way that\ncategorizations are done based on related data with respect to time and events.\nThis will help in building a more defined and conceptual systems for analysis\nof crime data that can easily be visualized and intelligently analyzed by\ncomputer systems.\n",
        "submitter": "Kester Quist-Aphetsi",
        "authors": "Quist-Aphetsi Kester",
        "comments": "12 pages. 2nd CMI and GTUC International Conference on Applications\n  of Mobile Communications in Africa: Prospects and Challenges, 2013, 2013",
        "journal-ref": "2nd CMI & GTUC International Conference on Applications of Mobile\n  Communications.pp1-12.2013",
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.CY",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "update_date": "2013-07-31",
        "authors_parsed": "Kester Quist-Aphetsi"
    },
    {
        "id": "1308.1605",
        "title": "The stability of a graph partition: A dynamics-based framework for\n  community detection",
        "abstract": "  Recent years have seen a surge of interest in the analysis of complex\nnetworks, facilitated by the availability of relational data and the\nincreasingly powerful computational resources that can be employed for their\nanalysis. Naturally, the study of real-world systems leads to highly complex\nnetworks and a current challenge is to extract intelligible, simplified\ndescriptions from the network in terms of relevant subgraphs, which can provide\ninsight into the structure and function of the overall system.\n  Sparked by seminal work by Newman and Girvan, an interesting line of research\nhas been devoted to investigating modular community structure in networks,\nrevitalising the classic problem of graph partitioning.\n  However, modular or community structure in networks has notoriously evaded\nrigorous definition. The most accepted notion of community is perhaps that of a\ngroup of elements which exhibit a stronger level of interaction within\nthemselves than with the elements outside the community. This concept has\nresulted in a plethora of computational methods and heuristics for community\ndetection. Nevertheless a firm theoretical understanding of most of these\nmethods, in terms of how they operate and what they are supposed to detect, is\nstill lacking to date.\n  Here, we will develop a dynamical perspective towards community detection\nenabling us to define a measure named the stability of a graph partition. It\nwill be shown that a number of previously ad-hoc defined heuristics for\ncommunity detection can be seen as particular cases of our method providing us\nwith a dynamic reinterpretation of those measures. Our dynamics-based approach\nthus serves as a unifying framework to gain a deeper understanding of different\naspects and problems associated with community detection and allows us to\npropose new dynamically-inspired criteria for community structure.\n",
        "submitter": "Jean-Charles Delvenne",
        "authors": "Jean-Charles Delvenne, Michael T. Schaub, Sophia N. Yaliraki, and\n  Mauricio Barahona",
        "comments": "3 figures; published as book chapter",
        "journal-ref": "Dynamics On and Of Complex Networks, Volume 2, pp 221-242,\n  Springer 2013",
        "doi": "10.1007/978-1-4614-6729-8_11",
        "report-no": NaN,
        "categories": "physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-08-08",
        "authors_parsed": "Delvenne Jean-Charles; Schaub Michael T.; Yaliraki Sophia N.; Barahona Mauricio"
    },
    {
        "id": "1310.3311",
        "title": "Investigation of Rule 73 as Case Study of Class 4 Long-Distance Cellular\n  Automata",
        "abstract": "  Cellular automata (CA) have been utilized for decades as discrete models of\nmany physical, mathematical, chemical, biological, and computing systems. The\nmost widely known form of CA, the elementary cellular automaton (ECA), has been\nstudied in particular due to its simple form and versatility. However, these\ndynamic computation systems possess evolutionary rules dependent on a\nneighborhood of adjacent cells, which limits their sampling radius and the\nenvironments that they can be used in.\n  The purpose of this study was to explore the complex nature of\none-dimensional CA in configurations other than that of the standard ECA.\nNamely, \"long-distance cellular automata\" (LDCA), a construct that had been\ndescribed in the past, but never studied. I experimented with a class of LDCA\nthat used spaced sample cells unlike ECA, and were described by the notation\nLDCA-x-y-n, where x and y represented the amount of spacing between the cell\nand its left and right neighbors, and n denoted the length of the initial tape\nfor tapes of finite size. Some basic characteristics of ECA are explored in\nthis paper, such as seemingly universal behavior, the prevalence of complexity\nwith varying neighborhoods, and qualitative behavior as a function of x and y\nspacing.\n  Focusing mainly on purely Class 4 behavior in LDCA-1-2, I found that Rule 73\ncould potentially be Turing universal through the emulation of a cyclic tag\nsystem, and revealed a connection between the mathematics of binary trees and\nEulerian numbers that might provide insight into unsolved problems in both\nfields.\n",
        "submitter": "Lucas Kang",
        "authors": "Lucas Kang",
        "comments": "23 pages (including references and comments), 25 figures, independent\n  research, to be published in nlin.CG, nlin.PS and cs.CC",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "nlin.CG cs.CC nlin.PS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-10-15",
        "authors_parsed": "Kang Lucas"
    },
    {
        "id": "1310.6654",
        "title": "Pseudo vs. True Defect Classification in Printed Circuits Boards using\n  Wavelet Features",
        "abstract": "  In recent years, Printed Circuit Boards (PCB) have become the backbone of a\nlarge number of consumer electronic devices leading to a surge in their\nproduction. This has made it imperative to employ automatic inspection systems\nto identify manufacturing defects in PCB before they are installed in the\nrespective systems. An important task in this regard is the classification of\ndefects as either true or pseudo defects, which decides if the PCB is to be\nre-manufactured or not. This work proposes a novel approach to detect most\ncommon defects in the PCBs. The problem has been approached by employing highly\ndiscriminative features based on multi-scale wavelet transform, which are\nfurther boosted by using a kernalized version of the support vector machines\n(SVM). A real world printed circuit board dataset has been used for\nquantitative analysis. Experimental results demonstrated the efficacy of the\nproposed method.\n",
        "submitter": "Sahil  Sikka",
        "authors": "Sahil Sikka and Karan Sikka and M.K. Bhuyan and Yuji Iwahori",
        "comments": "6 pages, 8 figures",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2013-10-25",
        "authors_parsed": "Sikka Sahil; Sikka Karan; Bhuyan M. K.; Iwahori Yuji"
    },
    {
        "id": "1312.379",
        "title": "Sample Complexity of Dictionary Learning and other Matrix Factorizations",
        "abstract": "  Many modern tools in machine learning and signal processing, such as sparse\ndictionary learning, principal component analysis (PCA), non-negative matrix\nfactorization (NMF), $K$-means clustering, etc., rely on the factorization of a\nmatrix obtained by concatenating high-dimensional vectors from a training\ncollection. While the idealized task would be to optimize the expected quality\nof the factors over the underlying distribution of training vectors, it is\nachieved in practice by minimizing an empirical average over the considered\ncollection. The focus of this paper is to provide sample complexity estimates\nto uniformly control how much the empirical average deviates from the expected\ncost function. Standard arguments imply that the performance of the empirical\npredictor also exhibit such guarantees. The level of genericity of the approach\nencompasses several possible constraints on the factors (tensor product\nstructure, shift-invariance, sparsity \\ldots), thus providing a unified\nperspective on the sample complexity of several widely used matrix\nfactorization schemes. The derived generalization bounds behave proportional to\n$\\sqrt{\\log(n)/n}$ w.r.t.\\ the number of samples $n$ for the considered matrix\nfactorization techniques.\n",
        "submitter": "Remi Gribonval",
        "authors": "R\\'emi Gribonval (INRIA - IRISA), Rodolphe Jenatton (INRIA Paris -\n  Rocquencourt, CMAP), Francis Bach (INRIA Paris - Rocquencourt, LIENS), Martin\n  Kleinsteuber (TUM), Matthias Seibert (TUM)",
        "comments": "to appear",
        "journal-ref": "IEEE Transactions on Information Theory, Institute of Electrical\n  and Electronics Engineers (IEEE), 2015, pp.18",
        "doi": NaN,
        "report-no": NaN,
        "categories": "stat.ML cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-04-10",
        "authors_parsed": "Gribonval R\u00e9mi INRIA - IRISA; Jenatton Rodolphe INRIA Paris -\n  Rocquencourt, CMAP; Bach Francis INRIA Paris - Rocquencourt, LIENS; Kleinsteuber Martin TUM; Seibert Matthias TUM"
    },
    {
        "id": "1401.669",
        "title": "Spatial DCT-Based Channel Estimation in Multi-Antenna Multi-Cell\n  Interference Channels",
        "abstract": "  This work addresses channel estimation in multiple antenna multicell\ninterference-limited networks. Channel state information (CSI) acquisition is\nvital for interference mitigation. Wireless networks often suffer from\nmulticell interference, which can be mitigated by deploying beamforming to\nspatially direct the transmissions. The accuracy of the estimated CSI plays an\nimportant role in designing accurate beamformers that can control the amount of\ninterference created from simultaneous spatial transmissions to mobile users.\nTherefore, a new technique based on the structure of the spatial covariance\nmatrix and the discrete cosine transform (DCT) is proposed to enhance channel\nestimation in the presence of interference. Bayesian estimation and Least\nSquares estimation frameworks are introduced by utilizing the DCT to separate\nthe overlapping spatial paths that create the interference. The spatial domain\nis thus exploited to mitigate the contamination which is able to discriminate\nacross interfering users. Gains over conventional channel estimation techniques\nare presented in our simulations which are also valid for a small number of\nantennas.\n",
        "submitter": "Maha Alodeh",
        "authors": "Maha Alodeh, Symeon Chatzinotas, Bjorn Ottersten",
        "comments": "Submitted for possible publication. arXiv admin note: text overlap\n  with arXiv:1203.5924 by other authors",
        "journal-ref": "IEEE Transactions on Signal Processing, (Volume:63 , Issue: 6 ),\n  pp.1404 - 1418, March 2015",
        "doi": "10.1109/TSP.2015.2393844",
        "report-no": NaN,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2015-05-11",
        "authors_parsed": "Alodeh Maha; Chatzinotas Symeon; Ottersten Bjorn"
    },
    {
        "id": "1402.1324",
        "title": "UCAT: Ubiquitous Context Awareness Tools for The Blind",
        "abstract": "  Visually impaired people are often confronted with new environments and they\nfind themselves face to face with an innumerous amount of difficulties when\nfacing these environments. Having to surpass and deal with these difficulties\nthat arise with their condition is something that we can help diminish. They\nare one sense down when trying to understand their surrounding environments and\ngather information about what is happening around them. Nowadays, mobile\ndevices present significant computing and technological capacity which has been\nincreasing to the point where it is very common for most people to have access\nto a device with Bluetooth, GPS, Wi-Fi, and both high processing and storage\ncapacities. This allows us to think of applications that can do so much to help\npeople with difficulties. In the particular case of blind people, the lack of\nvisual information can be bypassed with other contextual information retrieved\nby their own personal devices. Our goal is to provide information to blind\nusers, be able to give them information about the context that surrounds them.\nWe wanted to provide the blind users with the tools to create information and\nbe able to share this information between each other, information about people,\nlocations or objects. Our approach was to split the project into a data and\ninformation gathering phase where we did our field search and interviewed and\nelaborated on how is the situation of environment perception for blind users,\nfollowed by a technical phase where we implement a system based on the first\nstage. Our results gathered from both the collecting phase and our implementing\nphase showed that there is potential to use these tools in the blind community\nand that they welcome the possibilities and horizons that it opens them.\n",
        "submitter": "Tiago Guerreiro",
        "authors": "Ivo Rafael",
        "comments": "93 pages, MSc Thesis, University of Lisbon",
        "journal-ref": NaN,
        "doi": NaN,
        "report-no": NaN,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "update_date": "2014-02-07",
        "authors_parsed": "Rafael Ivo"
    }
]